# fetch 100 files 100.txt.
# remove duplicate and longue lines foreach line
# mix files
# sort files
# count how lines is duplicate
# count duclicat eline by function `count_of_duplicate`
- get top 3 for `command`

# if research is too big and lent
  - remove lines that duplicate just few time < 10 or 100

- search in generate dic/sqlite/table
  - awk '/$1==word1/ rate=$1 line=$3 {print $line}'

- score by duplicate with users and foreach users
